world size: 1, world rank: 0, local rank: 0, device_count: 1
Not using distributed mode
[06/05 16:13:42.461]: git:
  sha: 723ddb80e95dbd3c9a2e4ef68c1e0fa9319167cf, status: has uncommited changes, branch: main

[06/05 16:13:42.462]: Command: main.py -m=dab_detr --output_dir=logs/DABDETR_ARTIFICIAL_PLATE/R50/plates_5000 --batch_size=1 --epochs=100 --lr_drop=40 --number_of_classes_new=103 --resume=/fs/pool/pool-lambacher/Cluster_Projects/DAB-DETR/download_checkpoint/checkpoint.pth --coco_path=/fs/pool/pool-lambacher/Cluster_Projects/data/plates_5000
[06/05 16:13:42.463]: Full config saved to logs/DABDETR_ARTIFICIAL_PLATE/R50/plates_5000/config.json
[06/05 16:13:42.463]: world size: 1
[06/05 16:13:42.463]: rank: 0
[06/05 16:13:42.463]: local_rank: 0
[06/05 16:13:42.463]: args: Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=1, weight_decay=0.0001, epochs=100, lr_drop=40, save_checkpoint_interval=100, clip_max_norm=0.1, modelname='dab_detr', frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, batch_norm_type='FrozenBatchNorm2d', return_interm_layers=False, backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=300, pre_norm=False, num_select=300, transformer_activation='prelu', num_patterns=0, random_refpoints_xy=False, two_stage=False, num_feature_levels=4, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=1, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, focal_alpha=0.25, dataset_file='coco', coco_path='/fs/pool/pool-lambacher/Cluster_Projects/data/plates_5000', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DABDETR_ARTIFICIAL_PLATE/R50/plates_5000', note='', device='cuda', seed=42, resume='/fs/pool/pool-lambacher/Cluster_Projects/DAB-DETR/download_checkpoint/checkpoint.pth', number_of_classes_saved=91, number_of_classes_new=103, pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=False, num_workers=10, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, gpu=0, distributed=False)

Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=1, weight_decay=0.0001, epochs=100, lr_drop=40, save_checkpoint_interval=100, clip_max_norm=0.1, modelname='dab_detr', frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, batch_norm_type='FrozenBatchNorm2d', return_interm_layers=False, backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=300, pre_norm=False, num_select=300, transformer_activation='prelu', num_patterns=0, random_refpoints_xy=False, two_stage=False, num_feature_levels=4, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=1, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, focal_alpha=0.25, dataset_file='coco', coco_path='/fs/pool/pool-lambacher/Cluster_Projects/data/plates_5000', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DABDETR_ARTIFICIAL_PLATE/R50/plates_5000', note='', device='cuda', seed=42, resume='/fs/pool/pool-lambacher/Cluster_Projects/DAB-DETR/download_checkpoint/checkpoint.pth', number_of_classes_saved=91, number_of_classes_new=103, pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=False, num_workers=10, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, gpu=0, distributed=False)
[06/05 16:13:43.201]: number of params:43449117
[06/05 16:13:43.202]: params:
{
  "transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.0.activation.weight": 1,
  "transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.1.activation.weight": 1,
  "transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.2.activation.weight": 1,
  "transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.3.activation.weight": 1,
  "transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.4.activation.weight": 1,
  "transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.encoder.layers.5.activation.weight": 1,
  "transformer.encoder.query_scale.layers.0.weight": 65536,
  "transformer.encoder.query_scale.layers.0.bias": 256,
  "transformer.encoder.query_scale.layers.1.weight": 65536,
  "transformer.encoder.query_scale.layers.1.bias": 256,
  "transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.0.activation.weight": 1,
  "transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.1.activation.weight": 1,
  "transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.2.activation.weight": 1,
  "transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.3.activation.weight": 1,
  "transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.4.activation.weight": 1,
  "transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.layers.5.activation.weight": 1,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.query_scale.layers.0.weight": 65536,
  "transformer.decoder.query_scale.layers.0.bias": 256,
  "transformer.decoder.query_scale.layers.1.weight": 65536,
  "transformer.decoder.query_scale.layers.1.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.layers.2.bias": 4,
  "class_embed.weight": 23296,
  "class_embed.bias": 91,
  "refpoint_embed.weight": 1200,
  "input_proj.weight": 524288,
  "input_proj.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
loading annotations into memory...
Done (t=0.21s)
creating index...
index created!
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Start training
Epoch: [50]  [   0/5000]  eta: 4:35:32  lr: 0.000010  class_error: 100.00  loss: 238.8940 (238.8940)  loss_ce: 42.3968 (42.3968)  loss_bbox: 0.1589 (0.1589)  loss_giou: 0.6331 (0.6331)  loss_ce_0: 41.0947 (41.0947)  loss_bbox_0: 0.1832 (0.1832)  loss_giou_0: 0.6972 (0.6972)  loss_ce_1: 36.0054 (36.0054)  loss_bbox_1: 0.1714 (0.1714)  loss_giou_1: 0.6579 (0.6579)  loss_ce_2: 39.6616 (39.6616)  loss_bbox_2: 0.1578 (0.1578)  loss_giou_2: 0.6480 (0.6480)  loss_ce_3: 37.7738 (37.7738)  loss_bbox_3: 0.1567 (0.1567)  loss_giou_3: 0.6101 (0.6101)  loss_ce_4: 37.0581 (37.0581)  loss_bbox_4: 0.1656 (0.1656)  loss_giou_4: 0.6637 (0.6637)  loss_ce_unscaled: 42.3968 (42.3968)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0318 (0.0318)  loss_giou_unscaled: 0.3165 (0.3165)  loss_xy_unscaled: 0.0034 (0.0034)  loss_hw_unscaled: 0.0284 (0.0284)  cardinality_error_unscaled: 287.0000 (287.0000)  loss_ce_0_unscaled: 41.0947 (41.0947)  loss_bbox_0_unscaled: 0.0366 (0.0366)  loss_giou_0_unscaled: 0.3486 (0.3486)  loss_xy_0_unscaled: 0.0063 (0.0063)  loss_hw_0_unscaled: 0.0303 (0.0303)  cardinality_error_0_unscaled: 284.0000 (284.0000)  loss_ce_1_unscaled: 36.0054 (36.0054)  loss_bbox_1_unscaled: 0.0343 (0.0343)  loss_giou_1_unscaled: 0.3289 (0.3289)  loss_xy_1_unscaled: 0.0041 (0.0041)  loss_hw_1_unscaled: 0.0302 (0.0302)  cardinality_error_1_unscaled: 268.0000 (268.0000)  loss_ce_2_unscaled: 39.6616 (39.6616)  loss_bbox_2_unscaled: 0.0316 (0.0316)  loss_giou_2_unscaled: 0.3240 (0.3240)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0287 (0.0287)  cardinality_error_2_unscaled: 270.0000 (270.0000)  loss_ce_3_unscaled: 37.7738 (37.7738)  loss_bbox_3_unscaled: 0.0313 (0.0313)  loss_giou_3_unscaled: 0.3050 (0.3050)  loss_xy_3_unscaled: 0.0035 (0.0035)  loss_hw_3_unscaled: 0.0278 (0.0278)  cardinality_error_3_unscaled: 286.0000 (286.0000)  loss_ce_4_unscaled: 37.0581 (37.0581)  loss_bbox_4_unscaled: 0.0331 (0.0331)  loss_giou_4_unscaled: 0.3319 (0.3319)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0300 (0.0300)  cardinality_error_4_unscaled: 287.0000 (287.0000)  time: 3.3065  data: 1.8628  max mem: 1055
Epoch: [50]  [  10/5000]  eta: 0:46:51  lr: 0.000010  class_error: 100.00  loss: 426.0416 (705.5661)  loss_ce: 70.1339 (122.7512)  loss_bbox: 0.1589 (0.1617)  loss_giou: 0.6195 (0.6701)  loss_ce_0: 74.7509 (120.3021)  loss_bbox_0: 0.1974 (0.1991)  loss_giou_0: 0.7678 (0.7911)  loss_ce_1: 67.7435 (106.9622)  loss_bbox_1: 0.1714 (0.1706)  loss_giou_1: 0.7073 (0.7131)  loss_ce_2: 73.2190 (120.8577)  loss_bbox_2: 0.1578 (0.1630)  loss_giou_2: 0.6480 (0.6736)  loss_ce_3: 67.7200 (115.4446)  loss_bbox_3: 0.1520 (0.1598)  loss_giou_3: 0.6101 (0.6496)  loss_ce_4: 65.8682 (114.0694)  loss_bbox_4: 0.1492 (0.1616)  loss_giou_4: 0.6279 (0.6655)  loss_ce_unscaled: 70.1339 (122.7512)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0318 (0.0323)  loss_giou_unscaled: 0.3098 (0.3351)  loss_xy_unscaled: 0.0040 (0.0043)  loss_hw_unscaled: 0.0270 (0.0281)  cardinality_error_unscaled: 291.0000 (291.4545)  loss_ce_0_unscaled: 74.7509 (120.3021)  loss_bbox_0_unscaled: 0.0395 (0.0398)  loss_giou_0_unscaled: 0.3839 (0.3956)  loss_xy_0_unscaled: 0.0067 (0.0066)  loss_hw_0_unscaled: 0.0334 (0.0333)  cardinality_error_0_unscaled: 293.0000 (292.0000)  loss_ce_1_unscaled: 67.7435 (106.9622)  loss_bbox_1_unscaled: 0.0343 (0.0341)  loss_giou_1_unscaled: 0.3537 (0.3565)  loss_xy_1_unscaled: 0.0047 (0.0048)  loss_hw_1_unscaled: 0.0302 (0.0293)  cardinality_error_1_unscaled: 278.0000 (281.8182)  loss_ce_2_unscaled: 73.2190 (120.8577)  loss_bbox_2_unscaled: 0.0316 (0.0326)  loss_giou_2_unscaled: 0.3240 (0.3368)  loss_xy_2_unscaled: 0.0037 (0.0046)  loss_hw_2_unscaled: 0.0275 (0.0280)  cardinality_error_2_unscaled: 280.0000 (283.4545)  loss_ce_3_unscaled: 67.7200 (115.4446)  loss_bbox_3_unscaled: 0.0304 (0.0320)  loss_giou_3_unscaled: 0.3050 (0.3248)  loss_xy_3_unscaled: 0.0044 (0.0046)  loss_hw_3_unscaled: 0.0278 (0.0273)  cardinality_error_3_unscaled: 293.0000 (292.2727)  loss_ce_4_unscaled: 65.8682 (114.0694)  loss_bbox_4_unscaled: 0.0298 (0.0323)  loss_giou_4_unscaled: 0.3140 (0.3328)  loss_xy_4_unscaled: 0.0039 (0.0046)  loss_hw_4_unscaled: 0.0241 (0.0278)  cardinality_error_4_unscaled: 287.0000 (284.4545)  time: 0.5634  data: 0.1709  max mem: 1312
Epoch: [50]  [  20/5000]  eta: 0:34:12  lr: 0.000010  class_error: 100.00  loss: 265.6958 (557.1581)  loss_ce: 36.3638 (88.5707)  loss_bbox: 0.1613 (0.1664)  loss_giou: 0.6647 (0.6904)  loss_ce_0: 51.5603 (100.9418)  loss_bbox_0: 0.1946 (0.1968)  loss_giou_0: 0.7999 (0.7873)  loss_ce_1: 42.0568 (86.6011)  loss_bbox_1: 0.1711 (0.1776)  loss_giou_1: 0.7300 (0.7338)  loss_ce_2: 47.3619 (97.2229)  loss_bbox_2: 0.1667 (0.1707)  loss_giou_2: 0.6961 (0.7059)  loss_ce_3: 43.9377 (91.6874)  loss_bbox_3: 0.1605 (0.1665)  loss_giou_3: 0.6815 (0.6866)  loss_ce_4: 39.3641 (86.7999)  loss_bbox_4: 0.1531 (0.1659)  loss_giou_4: 0.6505 (0.6864)  loss_ce_unscaled: 36.3638 (88.5707)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0323 (0.0333)  loss_giou_unscaled: 0.3323 (0.3452)  loss_xy_unscaled: 0.0037 (0.0041)  loss_hw_unscaled: 0.0278 (0.0292)  cardinality_error_unscaled: 291.0000 (290.8095)  loss_ce_0_unscaled: 51.5603 (100.9418)  loss_bbox_0_unscaled: 0.0389 (0.0394)  loss_giou_0_unscaled: 0.4000 (0.3937)  loss_xy_0_unscaled: 0.0067 (0.0065)  loss_hw_0_unscaled: 0.0328 (0.0329)  cardinality_error_0_unscaled: 290.0000 (291.2857)  loss_ce_1_unscaled: 42.0568 (86.6011)  loss_bbox_1_unscaled: 0.0342 (0.0355)  loss_giou_1_unscaled: 0.3650 (0.3669)  loss_xy_1_unscaled: 0.0039 (0.0048)  loss_hw_1_unscaled: 0.0292 (0.0307)  cardinality_error_1_unscaled: 282.0000 (283.1905)  loss_ce_2_unscaled: 47.3619 (97.2229)  loss_bbox_2_unscaled: 0.0333 (0.0341)  loss_giou_2_unscaled: 0.3481 (0.3529)  loss_xy_2_unscaled: 0.0037 (0.0044)  loss_hw_2_unscaled: 0.0277 (0.0297)  cardinality_error_2_unscaled: 287.0000 (286.4762)  loss_ce_3_unscaled: 43.9377 (91.6874)  loss_bbox_3_unscaled: 0.0321 (0.0333)  loss_giou_3_unscaled: 0.3407 (0.3433)  loss_xy_3_unscaled: 0.0043 (0.0046)  loss_hw_3_unscaled: 0.0275 (0.0287)  cardinality_error_3_unscaled: 292.0000 (291.3333)  loss_ce_4_unscaled: 39.3641 (86.7999)  loss_bbox_4_unscaled: 0.0306 (0.0332)  loss_giou_4_unscaled: 0.3253 (0.3432)  loss_xy_4_unscaled: 0.0036 (0.0043)  loss_hw_4_unscaled: 0.0259 (0.0289)  cardinality_error_4_unscaled: 284.0000 (280.9048)  time: 0.2675  data: 0.0018  max mem: 1447
Epoch: [50]  [  30/5000]  eta: 0:29:16  lr: 0.000010  class_error: 100.00  loss: 175.9586 (475.0586)  loss_ce: 21.3216 (69.9217)  loss_bbox: 0.1779 (0.1748)  loss_giou: 0.7656 (0.7183)  loss_ce_0: 38.3088 (91.1558)  loss_bbox_0: 0.1799 (0.1923)  loss_giou_0: 0.7354 (0.7672)  loss_ce_1: 29.6023 (75.7351)  loss_bbox_1: 0.1779 (0.1849)  loss_giou_1: 0.7911 (0.7596)  loss_ce_2: 30.3990 (83.6129)  loss_bbox_2: 0.1867 (0.1776)  loss_giou_2: 0.7841 (0.7364)  loss_ce_3: 27.7522 (77.7423)  loss_bbox_3: 0.1825 (0.1763)  loss_giou_3: 0.7774 (0.7232)  loss_ce_4: 24.3354 (71.3836)  loss_bbox_4: 0.1807 (0.1758)  loss_giou_4: 0.7644 (0.7209)  loss_ce_unscaled: 21.3216 (69.9217)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0356 (0.0350)  loss_giou_unscaled: 0.3828 (0.3591)  loss_xy_unscaled: 0.0040 (0.0045)  loss_hw_unscaled: 0.0310 (0.0304)  cardinality_error_unscaled: 290.0000 (290.9355)  loss_ce_0_unscaled: 38.3088 (91.1558)  loss_bbox_0_unscaled: 0.0360 (0.0385)  loss_giou_0_unscaled: 0.3677 (0.3836)  loss_xy_0_unscaled: 0.0072 (0.0073)  loss_hw_0_unscaled: 0.0281 (0.0312)  cardinality_error_0_unscaled: 290.0000 (291.2581)  loss_ce_1_unscaled: 29.6023 (75.7351)  loss_bbox_1_unscaled: 0.0356 (0.0370)  loss_giou_1_unscaled: 0.3955 (0.3798)  loss_xy_1_unscaled: 0.0041 (0.0054)  loss_hw_1_unscaled: 0.0304 (0.0316)  cardinality_error_1_unscaled: 286.0000 (285.1290)  loss_ce_2_unscaled: 30.3990 (83.6129)  loss_bbox_2_unscaled: 0.0373 (0.0355)  loss_giou_2_unscaled: 0.3921 (0.3682)  loss_xy_2_unscaled: 0.0040 (0.0050)  loss_hw_2_unscaled: 0.0305 (0.0305)  cardinality_error_2_unscaled: 290.0000 (288.0000)  loss_ce_3_unscaled: 27.7522 (77.7423)  loss_bbox_3_unscaled: 0.0365 (0.0353)  loss_giou_3_unscaled: 0.3887 (0.3616)  loss_xy_3_unscaled: 0.0043 (0.0052)  loss_hw_3_unscaled: 0.0310 (0.0301)  cardinality_error_3_unscaled: 288.0000 (290.6452)  loss_ce_4_unscaled: 24.3354 (71.3836)  loss_bbox_4_unscaled: 0.0361 (0.0352)  loss_giou_4_unscaled: 0.3822 (0.3604)  loss_xy_4_unscaled: 0.0040 (0.0049)  loss_hw_4_unscaled: 0.0306 (0.0302)  cardinality_error_4_unscaled: 267.0000 (270.8710)  time: 0.2378  data: 0.0035  max mem: 1447
Epoch: [50]  [  40/5000]  eta: 0:27:42  lr: 0.000010  class_error: 100.00  loss: 143.1297 (392.7883)  loss_ce: 10.9297 (55.2540)  loss_bbox: 0.1948 (0.1785)  loss_giou: 0.7307 (0.7206)  loss_ce_0: 33.4478 (77.9095)  loss_bbox_0: 0.1727 (0.1865)  loss_giou_0: 0.6650 (0.7334)  loss_ce_1: 24.2725 (63.1834)  loss_bbox_1: 0.1914 (0.1865)  loss_giou_1: 0.7662 (0.7514)  loss_ce_2: 24.7675 (69.4035)  loss_bbox_2: 0.1891 (0.1790)  loss_giou_2: 0.7627 (0.7285)  loss_ce_3: 22.3712 (63.7749)  loss_bbox_3: 0.1987 (0.1823)  loss_giou_3: 0.7672 (0.7332)  loss_ce_4: 17.2649 (57.7742)  loss_bbox_4: 0.1941 (0.1810)  loss_giou_4: 0.7349 (0.7278)  loss_ce_unscaled: 10.9297 (55.2540)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0390 (0.0357)  loss_giou_unscaled: 0.3654 (0.3603)  loss_xy_unscaled: 0.0045 (0.0048)  loss_hw_unscaled: 0.0310 (0.0309)  cardinality_error_unscaled: 291.0000 (290.8293)  loss_ce_0_unscaled: 33.4478 (77.9095)  loss_bbox_0_unscaled: 0.0345 (0.0373)  loss_giou_0_unscaled: 0.3325 (0.3667)  loss_xy_0_unscaled: 0.0096 (0.0078)  loss_hw_0_unscaled: 0.0247 (0.0295)  cardinality_error_0_unscaled: 291.0000 (291.0732)  loss_ce_1_unscaled: 24.2725 (63.1834)  loss_bbox_1_unscaled: 0.0383 (0.0373)  loss_giou_1_unscaled: 0.3831 (0.3757)  loss_xy_1_unscaled: 0.0043 (0.0054)  loss_hw_1_unscaled: 0.0314 (0.0319)  cardinality_error_1_unscaled: 290.0000 (285.9024)  loss_ce_2_unscaled: 24.7675 (69.4035)  loss_bbox_2_unscaled: 0.0378 (0.0358)  loss_giou_2_unscaled: 0.3814 (0.3642)  loss_xy_2_unscaled: 0.0046 (0.0051)  loss_hw_2_unscaled: 0.0298 (0.0307)  cardinality_error_2_unscaled: 291.0000 (288.6098)  loss_ce_3_unscaled: 22.3712 (63.7749)  loss_bbox_3_unscaled: 0.0397 (0.0365)  loss_giou_3_unscaled: 0.3836 (0.3666)  loss_xy_3_unscaled: 0.0044 (0.0053)  loss_hw_3_unscaled: 0.0318 (0.0312)  cardinality_error_3_unscaled: 287.0000 (289.6341)  loss_ce_4_unscaled: 17.2649 (57.7742)  loss_bbox_4_unscaled: 0.0388 (0.0362)  loss_giou_4_unscaled: 0.3674 (0.3639)  loss_xy_4_unscaled: 0.0044 (0.0051)  loss_hw_4_unscaled: 0.0308 (0.0311)  cardinality_error_4_unscaled: 254.0000 (268.9512)  time: 0.2544  data: 0.0052  max mem: 1447
Epoch: [50]  [  50/5000]  eta: 0:25:56  lr: 0.000010  class_error: 100.00  loss: 130.5393 (356.2456)  loss_ce: 7.2384 (46.5151)  loss_bbox: 0.1948 (0.1830)  loss_giou: 0.7216 (0.7357)  loss_ce_0: 35.1092 (75.4731)  loss_bbox_0: 0.1405 (0.1785)  loss_giou_0: 0.6278 (0.7097)  loss_ce_1: 23.4171 (58.2208)  loss_bbox_1: 0.1993 (0.1892)  loss_giou_1: 0.6896 (0.7474)  loss_ce_2: 23.8000 (63.2295)  loss_bbox_2: 0.1870 (0.1819)  loss_giou_2: 0.6886 (0.7343)  loss_ce_3: 17.5475 (56.8136)  loss_bbox_3: 0.1987 (0.1868)  loss_giou_3: 0.7536 (0.7463)  loss_ce_4: 12.9855 (50.4734)  loss_bbox_4: 0.1952 (0.1852)  loss_giou_4: 0.7347 (0.7423)  loss_ce_unscaled: 7.2384 (46.5151)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0390 (0.0366)  loss_giou_unscaled: 0.3608 (0.3678)  loss_xy_unscaled: 0.0037 (0.0048)  loss_hw_unscaled: 0.0322 (0.0318)  cardinality_error_unscaled: 293.0000 (291.4510)  loss_ce_0_unscaled: 35.1092 (75.4731)  loss_bbox_0_unscaled: 0.0281 (0.0357)  loss_giou_0_unscaled: 0.3139 (0.3548)  loss_xy_0_unscaled: 0.0108 (0.0083)  loss_hw_0_unscaled: 0.0208 (0.0274)  cardinality_error_0_unscaled: 293.0000 (291.6471)  loss_ce_1_unscaled: 23.4171 (58.2208)  loss_bbox_1_unscaled: 0.0399 (0.0378)  loss_giou_1_unscaled: 0.3448 (0.3737)  loss_xy_1_unscaled: 0.0039 (0.0053)  loss_hw_1_unscaled: 0.0314 (0.0325)  cardinality_error_1_unscaled: 292.0000 (287.4118)  loss_ce_2_unscaled: 23.8000 (63.2295)  loss_bbox_2_unscaled: 0.0374 (0.0364)  loss_giou_2_unscaled: 0.3443 (0.3671)  loss_xy_2_unscaled: 0.0036 (0.0049)  loss_hw_2_unscaled: 0.0303 (0.0314)  cardinality_error_2_unscaled: 293.0000 (289.6667)  loss_ce_3_unscaled: 17.5475 (56.8136)  loss_bbox_3_unscaled: 0.0397 (0.0374)  loss_giou_3_unscaled: 0.3768 (0.3731)  loss_xy_3_unscaled: 0.0042 (0.0051)  loss_hw_3_unscaled: 0.0341 (0.0322)  cardinality_error_3_unscaled: 284.0000 (289.1373)  loss_ce_4_unscaled: 12.9855 (50.4734)  loss_bbox_4_unscaled: 0.0390 (0.0370)  loss_giou_4_unscaled: 0.3673 (0.3711)  loss_xy_4_unscaled: 0.0035 (0.0050)  loss_hw_4_unscaled: 0.0314 (0.0321)  cardinality_error_4_unscaled: 253.0000 (262.5098)  time: 0.2543  data: 0.0053  max mem: 1447
Epoch: [50]  [  60/5000]  eta: 0:24:15  lr: 0.000010  class_error: 100.00  loss: 104.8949 (321.2092)  loss_ce: 4.9672 (39.9134)  loss_bbox: 0.1859 (0.1795)  loss_giou: 0.7216 (0.7274)  loss_ce_0: 35.1092 (71.4747)  loss_bbox_0: 0.1428 (0.1769)  loss_giou_0: 0.6464 (0.7077)  loss_ce_1: 19.0990 (52.9872)  loss_bbox_1: 0.1704 (0.1839)  loss_giou_1: 0.6780 (0.7365)  loss_ce_2: 18.1347 (56.8942)  loss_bbox_2: 0.1762 (0.1794)  loss_giou_2: 0.7392 (0.7294)  loss_ce_3: 13.1334 (50.2635)  loss_bbox_3: 0.1834 (0.1844)  loss_giou_3: 0.7512 (0.7424)  loss_ce_4: 9.7307 (44.2131)  loss_bbox_4: 0.1814 (0.1816)  loss_giou_4: 0.7274 (0.7339)  loss_ce_unscaled: 4.9672 (39.9134)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0372 (0.0359)  loss_giou_unscaled: 0.3608 (0.3637)  loss_xy_unscaled: 0.0041 (0.0050)  loss_hw_unscaled: 0.0285 (0.0309)  cardinality_error_unscaled: 293.0000 (291.6721)  loss_ce_0_unscaled: 35.1092 (71.4747)  loss_bbox_0_unscaled: 0.0286 (0.0354)  loss_giou_0_unscaled: 0.3232 (0.3538)  loss_xy_0_unscaled: 0.0115 (0.0091)  loss_hw_0_unscaled: 0.0196 (0.0262)  cardinality_error_0_unscaled: 293.0000 (291.8361)  loss_ce_1_unscaled: 19.0990 (52.9872)  loss_bbox_1_unscaled: 0.0341 (0.0368)  loss_giou_1_unscaled: 0.3390 (0.3682)  loss_xy_1_unscaled: 0.0058 (0.0058)  loss_hw_1_unscaled: 0.0275 (0.0310)  cardinality_error_1_unscaled: 293.0000 (288.2951)  loss_ce_2_unscaled: 18.1347 (56.8942)  loss_bbox_2_unscaled: 0.0352 (0.0359)  loss_giou_2_unscaled: 0.3696 (0.3647)  loss_xy_2_unscaled: 0.0044 (0.0053)  loss_hw_2_unscaled: 0.0303 (0.0306)  cardinality_error_2_unscaled: 293.0000 (290.1639)  loss_ce_3_unscaled: 13.1334 (50.2635)  loss_bbox_3_unscaled: 0.0367 (0.0369)  loss_giou_3_unscaled: 0.3756 (0.3712)  loss_xy_3_unscaled: 0.0042 (0.0054)  loss_hw_3_unscaled: 0.0304 (0.0315)  cardinality_error_3_unscaled: 283.0000 (287.0820)  loss_ce_4_unscaled: 9.7307 (44.2131)  loss_bbox_4_unscaled: 0.0363 (0.0363)  loss_giou_4_unscaled: 0.3637 (0.3670)  loss_xy_4_unscaled: 0.0040 (0.0051)  loss_hw_4_unscaled: 0.0297 (0.0312)  cardinality_error_4_unscaled: 250.0000 (262.6721)  time: 0.2112  data: 0.0052  max mem: 1447
